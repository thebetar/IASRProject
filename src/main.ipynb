{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import librosa\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import argrelextrema\n",
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1: Read wav files and create numpy array of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TASK 1.1 Get voiced parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auto_correlation(clip, FRAME_SIZE = 100):\n",
    "    bins = np.zeros(FRAME_SIZE)\n",
    "    for i in range(0, FRAME_SIZE):\n",
    "        for j in range(0, FRAME_SIZE-i):\n",
    "            bins[i] += (clip[j] * clip[j+i])\n",
    "    return bins\n",
    "\n",
    "#doesnt work perfectly - it doesnt connect all parts of audio\n",
    "def get_voiced_parts(clip, sr, FRAME_SIZE = 100):\n",
    "    IS_CURR_VOICED = False\n",
    "\n",
    "    MAX_NUM_FRAMES = 25\n",
    "    START_NUM_FRAMES = 20\n",
    "    MIN_NUM_OF_SAMPLES_IN_SEGMENT = 0.2 * sr\n",
    "    MIN_RMS = 0.05\n",
    "\n",
    "    voiced_counter = 0\n",
    "    start_of_segment = -1\n",
    "    end_of_segment = -1\n",
    "    end_of_checking = (len(clip)//FRAME_SIZE-1) * FRAME_SIZE\n",
    "\n",
    "    for i in range(0, end_of_checking, FRAME_SIZE):\n",
    "        #for each frame\n",
    "        bins = np.zeros(FRAME_SIZE)\n",
    "        bins = get_auto_correlation(clip[i:i+FRAME_SIZE], FRAME_SIZE)\n",
    "        #plt.stem(bins)\n",
    "        maximums = np.diff(argrelextrema(bins, np.greater))\n",
    "        median_maximums = np.median(maximums)\n",
    "        #if median maximus in some boudaries\n",
    "        if median_maximums >= 7 and median_maximums <= 20 and np.std(maximums) < 7:\n",
    "            voiced_counter = min(voiced_counter + 1, MAX_NUM_FRAMES)\n",
    "\n",
    "            if not IS_CURR_VOICED:\n",
    "                voiced_counter = START_NUM_FRAMES\n",
    "                IS_CURR_VOICED = True\n",
    "                start_of_segment = i\n",
    "        else:\n",
    "            voiced_counter = max(0, voiced_counter - 1)\n",
    "\n",
    "            if voiced_counter == 0 and IS_CURR_VOICED:\n",
    "                IS_CURR_VOICED = False\n",
    "                end_of_segment = i\n",
    "                #if too short\n",
    "                if end_of_segment - start_of_segment < MIN_NUM_OF_SAMPLES_IN_SEGMENT:\n",
    "                    continue \n",
    "                rms = np.sqrt(np.mean(clip[start_of_segment:end_of_segment]**2))\n",
    "                if rms < MIN_RMS:\n",
    "                    continue\n",
    "                return clip[start_of_segment: end_of_segment]\n",
    "\n",
    "    end_of_segment = len(clip)\n",
    "    \n",
    "    if IS_CURR_VOICED and ((end_of_segment - start_of_segment) > MIN_NUM_OF_SAMPLES_IN_SEGMENT):\n",
    "        return clip[start_of_segment: end_of_segment]       \n",
    "    return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sample):\n",
    "    sample = sample / np.max(np.abs(sample))\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to detect spoken parts with outocorrleation\n",
    "new_sr = 8000\n",
    "clip, sr = librosa.load(\"../data/divide/anja_3.wav\")\n",
    "clip = librosa.resample(clip, orig_sr=sr, target_sr=new_sr)\n",
    "clip = librosa.to_mono(clip)\n",
    "clip = normalize(clip)\n",
    "\n",
    "IS_CURR_VOICED = False\n",
    "\n",
    "MAX_NUM_FRAMES = 25\n",
    "START_NUM_FRAMES = 20\n",
    "MIN_NUM_OF_SAMPLES_IN_SEGMENT = 0.2 * new_sr\n",
    "MIN_RMS = 0.05\n",
    "\n",
    "voiced_counter = 0\n",
    "start_of_segment = -1\n",
    "end_of_segment = -1\n",
    "FRAME_SIZE = 100\n",
    "len_clip_in_frames = len(clip) // FRAME_SIZE - 1\n",
    "end_of_checking = len_clip_in_frames * FRAME_SIZE\n",
    "out = np.zeros(len_clip_in_frames)\n",
    "curr_voiced_over_time = np.zeros(len_clip_in_frames)\n",
    "\n",
    "for i in range(0, end_of_checking, FRAME_SIZE):\n",
    "    #for each frame\n",
    "    bins = np.zeros(FRAME_SIZE)\n",
    "    bins = get_auto_correlation(clip[i:i+FRAME_SIZE], FRAME_SIZE)\n",
    "    #plt.stem(bins)\n",
    "    maximums = np.diff(argrelextrema(bins, np.greater))\n",
    "    median_maximums = np.median(maximums)\n",
    "    #if median maximus in some boudaries\n",
    "    if median_maximums >= 7 and median_maximums <= 20 and np.std(maximums) < 7:\n",
    "        out[i//FRAME_SIZE] = 1\n",
    "        voiced_counter = min(voiced_counter + 1,MAX_NUM_FRAMES)\n",
    "        if  not IS_CURR_VOICED:\n",
    "            voiced_counter = START_NUM_FRAMES\n",
    "            IS_CURR_VOICED = True\n",
    "            start_of_segment = i\n",
    "    else:\n",
    "        voiced_counter = max(0, voiced_counter-1)\n",
    "\n",
    "        if voiced_counter == 0 and IS_CURR_VOICED:\n",
    "            IS_CURR_VOICED = False\n",
    "            end_of_segment = i\n",
    "            #if too short\n",
    "            if end_of_segment - start_of_segment < MIN_NUM_OF_SAMPLES_IN_SEGMENT:\n",
    "                continue\n",
    "\n",
    "            rms = np.sqrt(np.mean(clip[start_of_segment:end_of_segment]**2))\n",
    "            if rms < MIN_RMS:\n",
    "                continue\n",
    "\n",
    "            a = clip[start_of_segment: end_of_segment]\n",
    "            print(start_of_segment/new_sr, end_of_segment/new_sr)\n",
    "            plt.axvline(start_of_segment, color='g')\n",
    "            plt.axvline(end_of_segment, color='g')\n",
    "\n",
    "    #plt.show() \n",
    "\n",
    "    curr_voiced_over_time[i//FRAME_SIZE] = voiced_counter\n",
    "end_of_segment = len(clip)\n",
    "if IS_CURR_VOICED and ((end_of_segment - start_of_segment) > MIN_NUM_OF_SAMPLES_IN_SEGMENT):\n",
    "    plt.axvline(start_of_segment, color='g')\n",
    "    plt.axvline(end_of_segment, color='g')\n",
    "    print(end_of_segment - start_of_segment)\n",
    "#print(start_of_segment/new_sr, end_of_segment/new_sr)\n",
    "plt.stem(np.repeat(out, FRAME_SIZE)*0.5, 'r')\n",
    "plt.plot(clip)\n",
    "\n",
    "plt.plot(np.repeat(curr_voiced_over_time, FRAME_SIZE)*0.1, 'k')\n",
    "sd.play(a, 8000)\n",
    "#print(maximums)\n",
    "#plt.plot(clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TASK 1.2 Normalize between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2. FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TASK 2.1 Preemphasis filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_preemphasis_filter(x, coeff=0.95):\n",
    "    # TODO - on our own??\n",
    "    x = librosa.effects.preemphasis(x, coef = coeff)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TASK 2.2 Create MFCC of samles of uniform size (30, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_features(y, sr = 8000, wanted_width = 40):\n",
    "    y = add_preemphasis_filter(y)\n",
    "    #20 row mfcc, 20 row delta mfcc, 1 row pitch...\n",
    "    windows_size = len(y) // (wanted_width - 1)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mels=20, n_fft=300, hop_length=windows_size, window='hamming')\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    mfcc = np.concatenate((mfcc, delta_mfcc, ), axis=0)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1d_features(y, sr = 8000, wanted_width = 5):\n",
    "    windows_size = len(y)//(wanted_width - 1)\n",
    "    #coeffients of polynomial fitting frequency information over time\n",
    "    polly_coeff = librosa.feature.poly_features(y=y, sr=sr, order=0, n_fft=1024, hop_length=windows_size, window='hamming')[0]\n",
    "    #how many times do signal cross zero\n",
    "    zero_cross = librosa.feature.zero_crossing_rate(y=y, frame_length=1024, hop_length=windows_size)[0]\n",
    "    #tone vs noise level in each windows\n",
    "    tone_vs_noise = librosa.feature.spectral_flatness(y=y, n_fft=1024, hop_length=windows_size)[0]\n",
    "    \n",
    "    return np.concatenate((polly_coeff, zero_cross, tone_vs_noise), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        TESTING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sr = 8000\n",
    "clip, sr = librosa.load(\"../data/eight/luka_3.wav\")\n",
    "clip = librosa.resample(clip, orig_sr=sr, target_sr=new_sr)\n",
    "clip = librosa.to_mono(clip)\n",
    "clip = normalize(clip)\n",
    "clip = get_voiced_parts(clip, new_sr)\n",
    "clip = add_preemphasis_filter(clip)\n",
    "features = get_2d_features(clip, new_sr)\n",
    "\n",
    "\n",
    "print(get_1d_features(clip))\n",
    "\n",
    "sd.play(clip, new_sr)\n",
    "sd.wait()\n",
    "plt.plot(clip)\n",
    "plt.show()\n",
    "plt.imshow(features, cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TASK 2.3 Other feutures -TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the example main function to create train/validate and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(directory, training_data_name = \"training_data.npy\"):\n",
    "    training_data = []\n",
    "    eye_in = 0\n",
    "    NUM_CATEGORIES = len(os.listdir(directory))\n",
    "    #get al subdirctories in directory\n",
    "    for category in os.listdir(directory):\n",
    "        print(category)\n",
    "        CATEGORIES.append(category)\n",
    "        path = os.path.join(directory, category)\n",
    "        for file in os.listdir(path):\n",
    "            try:\n",
    "                #Fs, audio = wavfile.read(os.path.join(path, file))\n",
    "                clip, sr = librosa.load(os.path.join(path, file))\n",
    "                clip = librosa.resample(clip, orig_sr=sr, target_sr=8000)\n",
    "                clip = librosa.to_mono(clip)\n",
    "\n",
    "                clip = normalize(clip)\n",
    "                clip = get_voiced_parts(clip,8000)\n",
    "\n",
    "                if(clip is None):\n",
    "                    print(\"{} not good\".format(os.path.join(path, file)))\n",
    "                    continue\n",
    "\n",
    "                features_2d = get_2d_features(clip)\n",
    "                features_1d = get_1d_features(clip)\n",
    "\n",
    "                training_data.append([features_2d, features_1d, np.eye(NUM_CATEGORIES)[eye_in]])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "        eye_in += 1\n",
    "    np.random.shuffle(training_data)\n",
    "    return np.array(training_data, dtype=object)\n",
    "\n",
    "CATEGORIES = []\n",
    "training_data = make_train_data(os.path.join(os.getcwd(), \"..\", \"data\"))\n",
    "\n",
    "np.save(\"training_data.npy\", training_data, allow_pickle=True)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 2\n",
    "plt.imshow(training_data[INDEX][0])\n",
    "print(training_data[INDEX][1])\n",
    "print(training_data[INDEX][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data for training\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the missing variables\n",
    "height = 0\n",
    "width = 0\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 16)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 8)\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        x = torch.randn(height, width).view(-1,1, height, width)\n",
    "        self._to_linear=None\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, len(CATEGORIES))\n",
    "\n",
    "    def convs(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2,2)) #pooling space 2x2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2,2)) #pooling space 2x2\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, (2,2)) #pooling space 2x2\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "# Initialize on GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU\")\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001) #learning rate\n",
    "napaka_f = nn.MSELoss()\n",
    "\n",
    "train_X = torch.Tensor([i[0] for i in training_data]).view(-1, height, width) #flatten\n",
    "#X = X/255.0 #normaliziram\n",
    "train_y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "\n",
    "#training\n",
    "BATCH_SIZE = 8\n",
    "EPOH = 3000\n",
    "\n",
    "for epoch in tqdm(range(EPOH)):\n",
    "    for i in range(0, len(train_X), BATCH_SIZE):\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,height,width)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        net.zero_grad() #damo zero gradiant\n",
    "        out = net(batch_X)\n",
    "        napaka = napaka_f(out, batch_y)\n",
    "        napaka.backward()\n",
    "        optimizer.step()\n",
    "    train_X, train_y = shuffle(train_X,train_y)\n",
    "print(napaka)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sounddevice as sd\n",
    "from scipy.io import wavfile \n",
    "\n",
    "freq = 44100\n",
    "duration = 4\n",
    "\n",
    "print(\"Recording: start speaking...\")\n",
    "recording = sd.rec(int(duration * freq), samplerate=freq, channels=2)\n",
    "\n",
    "sd.wait()\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), \"..\", \"data\", \"input\", \"input.wav\")\n",
    "wavfile.write(filepath, freq, recording)  # Fix the function call\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read new data and classify\n",
    "clip, sr = librosa.load(filepath)\n",
    "clip = librosa.resample(clip, orig_sr=sr, target_sr=8000)\n",
    "clip = librosa.to_mono(clip)\n",
    "clip = normalize(clip)\n",
    "clip = get_voiced_parts(clip,8000)\n",
    "clip = add_preemphasis_filter(clip)\n",
    "features = get_2d_features(clip, 8000)\n",
    "features = torch.Tensor(features).view(-1,1,height,width)\n",
    "features = features.to(device)\n",
    "out = net(features)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
